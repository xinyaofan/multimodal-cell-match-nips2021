{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18283,"status":"ok","timestamp":1672251019752,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"khP6NWrdKWSk","outputId":"09f0c2a3-e03f-4add-da58-8b7df8e68c50"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18799,"status":"ok","timestamp":1671416391460,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"SZFSNox2KiKN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d490e0bc-a774-478f-9117-d22be1984df2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 2.0 MB 24.6 MB/s \n","\u001b[K     |████████████████████████████████| 9.4 MB 91.9 MB/s \n","\u001b[K     |████████████████████████████████| 88 kB 9.4 MB/s \n","\u001b[K     |████████████████████████████████| 96 kB 6.2 MB/s \n","\u001b[K     |████████████████████████████████| 295 kB 97.1 MB/s \n","\u001b[K     |████████████████████████████████| 965 kB 83.2 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 71.1 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 2.6 MB/s \n","\u001b[?25h  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install scanpy --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7bo5YGlE7wI6"},"outputs":[],"source":["import torch\n","import random\n","from torch import nn\n","from torch.autograd import Variable\n","import anndata as ad\n","import numpy as np\n","import os\n","import gc\n","import collections \n","from argparse import Namespace\n","from torch.utils.data import Dataset, DataLoader\n","config = Namespace(\n","    ALPHA = 0.8,\n","    MARGIN = 0.5,\n","    N_CHANNELS = 64,\n","    LEARNING_RATE = 0.00002,\n","    \n","    DEVICE = 'cuda',\n","    BATCH_SIZE = 128,\n","    NUM_WORKERS = 4,\n","    N_GENES = 13431,\n","    N_PEAKS = 116465,\n","    MAX_SEQ_LEN_GEX = 1500,\n","    MAX_SEQ_LEN_ATAC = 15000,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDWcM83eKMAJ"},"outputs":[],"source":["# adata_gex = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/GEX_processed.h5ad\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HyD0b7IGp412"},"outputs":[],"source":["# adata_atac = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/ATAC_processed.h5ad\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcYnPeRzsPLH"},"outputs":[],"source":["def get_chr_index(adata_atac):\n","  r\"\"\"\n","  Output row indices for each chromosome for each chromosome\n","  Parameters\n","  ----------\n","  adata_atac\n","      annData for ATAC\n","  Returns\n","  -------\n","  chr_index\n","      Dictionary of indices for each chromosome\n","  \"\"\"\n","  row_name = adata_atac.var.index\n","  chr_name = [c.split(\"-\")[0] for c in row_name]\n","  lst = np.unique(chr_name) # names for chromosome\n","\n","  chr_index = dict()\n","  for i in range(len(lst)):\n","    index = [a for a, l in enumerate(chr_name) if l == lst[i]]\n","    if lst[i] not in chr_index:\n","      chr_index[lst[i]]=index\n","\n","  return chr_index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FlJMBCpbKIEa"},"outputs":[],"source":["## Write cnn modules for gex modalities\n","class gexCNN(nn.Module):\n","    \"\"\"customized  module\"\"\"\n","    #argument index is the poisition for each choromosome\n","    def __init__(self, kernel_size):\n","        super(gexCNN, self).__init__()\n","\n","        # Conv layer\n","        self.in_channels = 1 \n","        self.out_channels = config.N_CHANNELS\n","        self.kernel_size = kernel_size   \n","        self.stride = 50 # TO CHANGE \n","        self.padding = 25 # TO CHANGE\n","        self.pool_size = 2\n","        self.pool_stride = 1\n","        self.convs = nn.Sequential(\n","            nn.Conv1d(in_channels = self.in_channels, \n","                      out_channels = self.out_channels, \n","                      kernel_size = self.kernel_size,\n","                      stride = self.stride,\n","                      padding = self.padding),\n","            nn.LeakyReLU(),\n","            nn.MaxPool1d(kernel_size = self.pool_size,\n","                         stride = self.pool_stride)\n","        )\n","\n","        # # FC layer\n","        # self.conv_out_features = int((config.N_GENES + 2*self.padding - self.kernel_size) / self.stride + 1)\n","        # self.fc_in_features = int((self.conv_out_features - self.pool_size) / self.pool_stride + 1) * self.out_channels\n","        # self.fc_out_feature = 300\n","        # self.fc = nn.Linear(in_features = self.fc_in_features, out_features = self.fc_out_feature) \n","\n","    def forward(self, x):\n","        r\"\"\"  \n","        Generate GEX embeddings\n","        \n","        Parameters\n","        ----------\n","        x\n","            Pre-processed GEX data (batch_size x 1 x N_GENES)\n","        \n","        Returns\n","        -------\n","        gex_embed\n","            GEX embeddings of a batch (batch_size x seq_len x dim_size)\n","        \"\"\"\n","        gex_embed = self.convs(x.float())\n","        # gex_embed = torch.flatten(gex_embed, 1)\n","        # gex_embed = self.fc(gex_embed)\n","        return gex_embed.transpose(1,2).to(config.DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Fd0OvPg3PGo"},"outputs":[],"source":["# # Test for gexCNN()\n","# x = torch.tensor(np.asarray(csr_gex[:5].todense())).unsqueeze(1) # 5 cells\n","# print(x.size())\n","# model = gexCNN(kernel_size = 100)\n","# print(model(x).size())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LUWvJUcpKIHe"},"outputs":[],"source":["# Write cnn modules for atac modalities\n","class atacCNN(nn.Module):\n","    #argument index is the poisition for each choromosome\n","    def __init__(self, index, kernel_size_1, kernel_size_2):\n","        super(atacCNN, self).__init__()\n","        self.index = index\n","        \n","        # Conv layer\n","        self.in_channels_1 = 1 \n","        self.out_channels_1 = int(config.N_CHANNELS / 2)\n","        self.kernel_size_1 = kernel_size_1\n","        self.stride_1 = 10 # TO CHANGE \n","        self.padding_1 = 5 # TO CHANGE\n","\n","        self.in_channels_2 = int(config.N_CHANNELS / 2)\n","        self.out_channels_2 = config.N_CHANNELS \n","        self.kernel_size_2 = kernel_size_2\n","        self.stride_2 = 5 # TO CHANGE \n","        self.padding_2 = 3 # TO CHANGE\n","\n","        self.convs = nn.Sequential(\n","            nn.Conv1d(in_channels = self.in_channels_1, \n","                      out_channels = self.out_channels_1, \n","                      kernel_size = self.kernel_size_1,\n","                      stride = self.stride_1,\n","                      padding = self.padding_1),\n","            nn.LeakyReLU(),\n","            nn.MaxPool1d(kernel_size = 5, stride = 2),\n","\n","            nn.Conv1d(in_channels = self.in_channels_2, \n","                      out_channels = self.out_channels_2, \n","                      kernel_size = self.kernel_size_2,\n","                      stride = self.stride_2,\n","                      padding = self.padding_2),\n","            nn.LeakyReLU(),\n","            nn.MaxPool1d(kernel_size = 2, stride = 1)             \n","        )\n","\n","\n","\n","    def forward(self, x):\n","        r\"\"\"  \n","        Generate ATAC embeddings\n","        \n","        Parameters\n","        ----------\n","        x\n","            Pre-processed ATAC data (batch_size x 1 x N_PEAKS)\n","        \n","        Returns\n","        -------\n","        atac_embed\n","            ATAC embeddings of a batch (batch_size x seq_len x dim_size)\n","        \"\"\"\n","        atac_embed = []\n","        for chr in self.index.keys(): \n","            idx = self.index[chr]\n","            x_chr = x[:,:,idx]\n","            x_chr = self.convs(x_chr.float())\n","            atac_embed.append(x_chr)\n","        atac_embed = torch.cat(atac_embed, dim = 2)\n","        return atac_embed.transpose(1,2).to(config.DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"orbL3zsTqYPy"},"outputs":[],"source":["# # Test for ATAC_CNN()\n","# x = torch.tensor(np.asarray(csr_atac[:5].todense())).unsqueeze(1) # 5 cells\n","# print(x.size())\n","# # index = get_chr_index(adata_atac)\n","# model = atacCNN(kernel_size_1 = 50, kernel_size_2 = 10, index = index)\n","# print(model(x).size())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2cTz8oUD4aGx"},"outputs":[],"source":["class MultimodalAttention(nn.Module):\n","    def __init__(self):\n","        super(MultimodalAttention, self).__init__()\n","        self.nhead_gex = 1\n","        self.nhead_atac = 4\n","        self.nhead_multi = 4\n","        self.nlayer_gex = 1\n","        self.nlayer_atac = 1\n","        self.nlayer_multi = 1\n","\n","        self.encoder_layer_gex = nn.TransformerEncoderLayer(d_model = config.N_CHANNELS, nhead = self.nhead_gex)\n","        self.transformer_encoder_gex = nn.TransformerEncoder(self.encoder_layer_gex, num_layers = self.nlayer_gex)\n","        # self.linear_gex_0 = nn.LazyLinear(out_features = 1)\n","        self.linear1_gex_0 = nn.LazyLinear(out_features = 10)\n","        self.linear2_gex_0 = nn.LazyLinear(out_features = 300)\n","\n","\n","        self.encoder_layer_atac = nn.TransformerEncoderLayer(d_model = config.N_CHANNELS, nhead = self.nhead_atac)\n","        self.transformer_encoder_atac = nn.TransformerEncoder(self.encoder_layer_atac, num_layers = self.nlayer_atac)\n","        # self.linear_atac_0 = nn.LazyLinear(out_features = 1)\n","        self.linear1_atac_0 = nn.LazyLinear(out_features = 30)\n","        self.linear2_atac_0 = nn.LazyLinear(out_features = 300)\n","\n","        self.encoder_layer_multi = nn.TransformerEncoderLayer(d_model = config.N_CHANNELS, nhead = self.nhead_multi)\n","        self.transformer_encoder_multi = nn.TransformerEncoder(self.encoder_layer_multi, num_layers = self.nlayer_multi)\n","        # self.linear_gex_1 = nn.LazyLinear(out_features = 1)\n","        # self.linear_atac_1 = nn.LazyLinear(out_features = 1)\n","        self.linear1_gex_1 = nn.LazyLinear(out_features = 10)\n","        self.linear2_gex_1 = nn.LazyLinear(out_features = 300)\n","        self.linear1_atac_1 = nn.LazyLinear(out_features = 30)\n","        self.linear2_atac_1 = nn.LazyLinear(out_features = 300)\n","    \n","\n","    def forward(self, gex_embed, atac_embed):\n","      r\"\"\"  \n","      Incorporate two self-attention and one cross-attention module\n","\n","      Parameters\n","      ----------\n","      gex_embed\n","          GEX embeddings of a batch (batch_size x seq_len_gex x dim_size)\n","      atac_embed\n","          ATAC embeddings of a batch (batch_size x seq_len_atac x dim_size)\n","\n","      Returns\n","      -------\n","      ## TO FILL\n","      \"\"\"\n","      seq_len_gex = gex_embed.size()[1]\n","      seq_len_atac = atac_embed.size()[1]\n","\n","      gex_context = self.transformer_encoder_gex(gex_embed)\n","      atac_context = self.transformer_encoder_atac(atac_embed)\n","\n","      # Average self-attention fragment representation\n","      # gex_out_0 = self.linear_gex_0(gex_context.permute(0,2,1)).squeeze(2)\n","      # atac_out_0 = self.linear_atac_0(atac_context.permute(0,2,1)).squeeze(2)\n","      gex_out_0  = self.linear1_gex_0(gex_context.permute(0,2,1)).squeeze(2)\n","      # print(gex_out_0.size())\n","      gex_out_0  = self.linear2_gex_0(gex_out_0.flatten(start_dim=1))\n","      # print(gex_out_0.size())\n","      atac_out_0 = self.linear1_atac_0(atac_context.permute(0,2,1)).squeeze(2)\n","      # print(atac_out_0.size())\n","      atac_out_0 = self.linear2_atac_0(atac_out_0.flatten(start_dim=1))\n","      # print(atac_out_0.size())\n","\n","      multi_embed = torch.cat((gex_context, atac_context), dim = 1)\n","      multi_context = self.transformer_encoder_multi(multi_embed)\n","      \n","      multi_context_gex = multi_context[:, :seq_len_gex, :]\n","      multi_context_atac = multi_context[:, seq_len_gex:, :]\n","      \n","      # # Average cross-attention fragment representation\n","      # gex_out_1 = multi_context_gex.mean(dim = 1)\n","      # atac_out_1 = multi_context_atac.mean(dim = 1)\n","      gex_out_1  = self.linear1_gex_1(multi_context_gex.permute(0,2,1)).squeeze(2)\n","      gex_out_1  = self.linear2_gex_1(gex_out_1.flatten(start_dim=1))\n","      atac_out_1 = self.linear1_atac_1(multi_context_atac.permute(0,2,1)).squeeze(2)\n","      atac_out_1 = self.linear2_atac_1(atac_out_1.flatten(start_dim=1))\n","\n","      return gex_out_0.to(config.DEVICE), gex_out_1.to(config.DEVICE), atac_out_0.to(config.DEVICE), atac_out_1.to(config.DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bKfe5KqplRBE"},"outputs":[],"source":["# # index = get_chr_index(adata_atac)\n","\n","# x_gex = torch.tensor(np.asarray(csr_gex[:5].todense())).unsqueeze(1).to(config.DEVICE) # 5 cells\n","# x_atac = torch.tensor(np.asarray(csr_atac[:5].todense())).unsqueeze(1).to(config.DEVICE) # 5 cells\n","\n","# gex_cnn = gexCNN(kernel_size = 100).to(config.DEVICE)\n","# atac_cnn = atacCNN(kernel_size_1 = 50, kernel_size_2 = 10, index = index).to(config.DEVICE)\n","# multi_attention = MultimodalAttention().to(config.DEVICE)\n","\n","# gex_embed = gex_cnn(x_gex).to(config.DEVICE)\n","# atac_embed = atac_cnn(x_atac).to(config.DEVICE)\n","# print(gex_embed.size()); print(atac_embed.size())\n","# gex_out_0, gex_out_1, atac_out_0, atac_out_1 = multi_attention(gex_embed, atac_embed)\n","# print(atac_out_0.size())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpHCCxVo-1Bj"},"outputs":[],"source":["# embedding = nn.Embedding(1000, 128)\n","# anchor_ids = torch.randint(0, 1000, (1,))\n","# positive_ids = torch.randint(0, 1000, (1,))\n","# negative_ids = torch.randint(0, 1000, (1,))\n","# anchor = embedding(anchor_ids)\n","# positive = embedding(positive_ids)\n","# negative = embedding(negative_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5oscyv8g_gPF"},"outputs":[],"source":["# print(anchor.size())\n","# print(positive.size())\n","# print(negative.size())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZXU1s_kVtKUQ"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, kernel_size_gex, kernel_size_atac_1, kernel_size_atac_2, index):\n","        super(Encoder, self).__init__()\n","\n","        self.kernel_size_gex = kernel_size_gex\n","        self.kernel_size_atac_1 = kernel_size_atac_1\n","        self.kernel_size_atac_2 = kernel_size_atac_2\n","        self.index = index\n","\n","        self.gex_cnn = gexCNN(kernel_size = self.kernel_size_gex)\n","        self.atac_cnn = atacCNN(kernel_size_1 = self.kernel_size_atac_1, kernel_size_2 = self.kernel_size_atac_2, index = self.index)\n","        self.multi_attention = MultimodalAttention()\n","\n","        \n","    def forward(self, x_gex, x_atac):\n","\n","        gex_embed = self.gex_cnn(x_gex)\n","        atac_embed = self.atac_cnn(x_atac)\n","        gex_out_0, gex_out_1, atac_out_0, atac_out_1 = self.multi_attention(gex_embed, atac_embed)\n","\n","        return gex_out_0, gex_out_1, atac_out_0, atac_out_1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ql7ebMHlvjho"},"outputs":[],"source":["# gex_test = torch.tensor(np.asarray(adata_gex.layers['log_norm'][:5].todense())).unsqueeze(1) # 5 cells\n","# atac_test = torch.tensor(np.asarray(adata_atac.layers['log_norm'][:5].todense())).unsqueeze(1) # 5 cells\n","\n","# index = get_chr_index(adata_atac)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8JVH48uxfQU"},"outputs":[],"source":["# encoder = Encoder(kernel_size = 32, index = index )\n","# gex_out_0, gex_out_1, atac_out_0, atac_out_1 = encoder(gex_test, atac_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aq9aH7Nc_d51"},"outputs":[],"source":["from numpy.lib.shape_base import row_stack\n","class bidirectTripletLoss(nn.Module):\n","    r\"\"\"\n","    \n","    Output bidirectional triplet loss for two pairs of gex and atac\n","    ----------\n","    gex_0_mat: Matrix of GEX embeddings from self-attention (batch_size x embedding_size_0)\n","\n","    Returns\n","    -------\n","    loss\n","    \"\"\"\n","    def __init__(self, alpha, margin):\n","        super(bidirectTripletLoss, self).__init__()\n","\n","        self.alpha = alpha\n","        self.margin = margin\n","        self.cross_entropy_loss = nn.CrossEntropyLoss()\n","\n","    def similarityScore(self, gex_out_0, gex_out_1, atac_out_0, atac_out_1):\n","        r\"\"\"\n","        Output similarity scores for two pairs of gex and atac\n","        ----------\n","\n","\n","        Returns\n","        -------\n","        score: batch_size * batch_size\n","        similarity score between two modalities\n","        \"\"\" \n","\n","        # print(gex_mat.size())\n","        # print(atac_mat.size())\n","\n","        # gex_mat0, gex_mat1 = torch.split(gex_mat, config.N_CHANNELS, dim = 1)\n","        # atac_mat0, atac_mat1 = torch.split(atac_mat,config.N_CHANNELS, dim = 1)\n","\n","        gex_out_0 = nn.functional.normalize(gex_out_0, dim = 1)\n","        gex_out_1 = nn.functional.normalize(gex_out_1, dim = 1)\n","        atac_out_0 = nn.functional.normalize(atac_out_0, dim = 1)\n","        atac_out_1 = nn.functional.normalize(atac_out_1, dim = 1)\n","        \n","        score = torch.mm(gex_out_0, atac_out_0.transpose(0,1)) + self.alpha * torch.mm(gex_out_1, atac_out_1.transpose(0,1))\n","        return  score.to(config.DEVICE)\n","\n","    # def triplet(self, score_mat):\n","\n","    #     batch_size = score_mat.size()[0]       \n","    #     true_score = torch.diagonal(score_mat)\n","    #     # print(\"true_score:\\n\", true_score)\n","    #     # print(\"true_score dimension:\\n\", true_score.size())\n","        \n","    #     reduced_score_mat = score_mat - torch.diag(true_score) # set the diagnoal score to be zero\n","        \n","    #     neg_index_1 = torch.argmax(reduced_score_mat, dim = 1) # indices of hard negatives for GEX\n","    #     neg_index_2 = torch.argmax(reduced_score_mat, dim = 0) # indices of hard negatives for ATAC\n","    #     # print(\"neg_index_1:\\n\", neg_index_1); print(\"neg_index_2:\\n\", neg_index_2)\n","    #     # print(\"neg_index_1 dimension:\\n\", neg_index_1.size()); print(\"neg_index_2 dimension:\\n\", neg_index_2.size())\n","    #     neg_1 = score_mat[[range(batch_size), neg_index_1]] # hard negatives for GEX \n","    #     neg_2 = score_mat[[neg_index_2, range(batch_size)]] # hard negatives for ATAC\n","    #     # print(\"neg_1:\\n\", neg_1); print(\"neg_2:\\n\", neg_2)\n","        \n","    #     loss_1 = torch.max(self.margin - true_score + neg_1, torch.zeros(1, batch_size).to(config.DEVICE))\n","    #     loss_2 = torch.max(self.margin - true_score + neg_2, torch.zeros(1, batch_size).to(config.DEVICE))\n","\n","    #     return torch.mean(loss_1 + loss_2)\n","    def idxSemiHardRow(self, score_mat):\n","\n","        batch_size = score_mat.size()[0]       \n","        true_score = torch.diagonal(score_mat)\n","        \n","        reduced_score_mat = score_mat - torch.diag(true_score) # set the diagnoal score to be zero\n","        idx_hardneg = torch.argmax(reduced_score_mat, dim = 1) # indices of hard negatives\n","\n","        # Sample from negatives with similarity score following positive - margin < nagative < positive\n","        row_scaled = score_mat - true_score.view(-1, 1) # each row scaled by diagonal values\n","        row_logical = torch.logical_and(row_scaled < 0, row_scaled > - self.margin) # need to sample from Trues\n","        iidx_sample_from = [[j for j in range(batch_size) if row_logical[i].tolist()[j]] for i in range(batch_size)] # indices that can be sampled from for each row\n","        # print('iidx_sample_from', iidx_sample_from)\n","\n","        idx_semi_hard = []\n","        for i in range(batch_size):\n","            idx = iidx_sample_from[i]\n","            if len(idx) > 0: \n","                idx_semi_hard.append(random.sample(idx, 1)[0])\n","            else:\n","                idx_semi_hard.append(idx_hardneg[i])\n","        return idx_semi_hard\n","\n","    def triplet(self, score_mat):\n","\n","        # print(score_mat)\n","        batch_size = score_mat.size()[0]       \n","        true_score = torch.diagonal(score_mat)\n","        # print(\"true_score:\\n\", true_score)\n","        # print(\"true_score dimension:\\n\", true_score.size())\n","\n","        neg_index_1 = self.idxSemiHardRow(score_mat)\n","        neg_index_2 = self.idxSemiHardRow(score_mat.T)\n","        # print(\"neg_index_1:\\n\", neg_index_1); print(\"neg_index_2:\\n\", neg_index_2)\n","        neg_1 = score_mat[[range(batch_size), neg_index_1]] # hard negatives for GEX \n","        neg_2 = score_mat[[neg_index_2, range(batch_size)]] # hard negatives for ATAC\n","        # print(\"neg_1:\\n\", neg_1); print(\"neg_2:\\n\", neg_2)\n","        \n","        loss_1 = torch.max(self.margin - true_score + neg_1, torch.zeros(1, batch_size).to(config.DEVICE))\n","        loss_2 = torch.max(self.margin - true_score + neg_2, torch.zeros(1, batch_size).to(config.DEVICE))\n","\n","        return torch.mean(loss_1 + loss_2)\n","\n","    def crossEntropy(self, score_mat):\n","\n","        batch_size = score_mat.size()[0]\n","        target = torch.arange(batch_size)\n","\n","        loss_1 = self.cross_entropy_loss(score_mat, target.to(config.DEVICE))\n","        loss_2 = self.cross_entropy_loss(score_mat.T, target.to(config.DEVICE))\n","\n","        return 0.5 * (loss_1 + loss_2)\n","\n","    def cellTypeMatchingProbRow(self, score_mat, cell_type):\n","\n","        # Collect list of index list for each cell type\n","        idx_in_type = collections.defaultdict(list)\n","        for i, x in enumerate(cell_type):\n","            idx_in_type[x].append(i)\n","\n","        # Compute matching probs for each cell type\n","        score_mat_norm = score_mat.softmax(dim = 0)\n","        probs = []\n","        for idx in idx_in_type.values():\n","            prob_type = 0\n","            for i in idx:\n","                prob_type += score_mat_norm[i, idx].sum()\n","            probs.append(prob_type / len(idx)) \n","\n","        # Take average of matching prob from cell types\n","        ct_match_prob = torch.tensor(probs).mean()\n","\n","        return ct_match_prob\n","\n","    def cellTypeMatchingProb(self, score_mat, cell_type):\n","        row = self.cellTypeMatchingProbRow(score_mat, cell_type) # Softmax on rows (normalize GEX)\n","        col = self.cellTypeMatchingProbRow(score_mat.T, cell_type) # Softmax on cols (normalize ATAC) \n","        \n","        return 0.5 * (row + col)\n","\n","    # def cellTypeMatchingProb(self, score_mat, cell_type):\n","\n","    #     # Compute matching probs for each cell type\n","    #     idx_in_type = collections.defaultdict(list)\n","    #     for i, x in enumerate(cell_type):\n","    #         idx_in_type[x].append(i)\n","\n","    #     sum_score_mat = torch.zeros(len(idx_in_type.values()),len(idx_in_type.values()))\n","    #     for i, dx in enumerate(idx_in_type.values()):\n","    #       for j, dx2 in enumerate(idx_in_type.values()):\n","    #         tem = score_mat[np.ix_(dx, dx2)].sum()\n","    #         sum_score_mat[i,j] = tem\n","\n","    #     score_mat_norm = 0.5 * (sum_score_mat.softmax(dim = 0) + sum_score_mat.softmax(dim = 1))\n","    #     # print('sum_score_mat:\\n', sum_score_mat)\n","    #     return torch.mean(torch.diagonal(score_mat_norm))\n","\n","    def forward(self, gex_out_0, gex_out_1, atac_out_0, atac_out_1, cell_type):\n","      \n","        score_mat = self.similarityScore(gex_out_0, gex_out_1, atac_out_0, atac_out_1)#; print(\"score_mat:\\n\", score_mat)\n","        # print('score_mat:\\n', score_mat)\n","        loss_triplet = self.triplet(score_mat)\n","        loss_cross = self.crossEntropy(score_mat)\n","        loss = loss_triplet + loss_cross\n","        # print(score_mat); print(cell_type)\n","        ct_match_prob = self.cellTypeMatchingProb(score_mat, cell_type)\n","\n","        return loss.to(config.DEVICE), loss_triplet, loss_cross, ct_match_prob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B3QiH33JZGL3"},"outputs":[],"source":["# TEST\n","# import random\n","# random.seed(0)\n","# gex_out_0 = torch.randn([5,64])\n","# gex_out_1 = torch.randn([5,64])\n","# atac_out_0 = torch.randn([5,64])\n","# atac_out_1 = torch.randn([5,64])\n","# cell_type=['a','a','b','a','b']\n","# loss = bidirectTripletLoss(alpha=0.2, margin=0.05)\n","# res = loss(gex_out_0, gex_out_1, atac_out_0, atac_out_1, cell_type)\n","# res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zpMYKKtHUYa5"},"outputs":[],"source":["# random.seed(1)\n","# score_mat = torch.randn([5,5])\n","# true_score = torch.diagonal(score_mat)\n","# margin = 3\n","# print(score_mat)\n","# print(true_score)\n","# print(true_score.view(-1, 1))\n","# row_scaled = score_mat - true_score.view(-1, 1) # each row scaled by diagonal values\n","# print(row_scaled)\n","# print(row_scaled < 0) \n","# print(row_scaled > -margin)\n","# row_logical = torch.logical_and(row_scaled < 0, row_scaled > -margin) # sample from negatives with similarity score following positive - margin < nagative < positive\n","# print(row_logical) \n","# print([[j for j in range(5) if row_logical[i].tolist()[j]] for i in range(5)]) # indices that can be sampled from for each row"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"InmmdcllfCcR"},"outputs":[],"source":["class MultiomeDataset(Dataset):\n","    def __init__(\n","        self, csr_gex, csr_atac, cell_type\n","    ):\n","        super().__init__()\n","        \n","        self.csr_gex = csr_gex\n","        self.csr_atac = csr_atac\n","        self.cell_type = cell_type\n","    \n","    def __len__(self):\n","        return self.csr_gex.shape[0]\n","    \n","    def __getitem__(self, index: int):\n","        x_gex = torch.tensor(self.csr_gex[index,:].todense())\n","        x_atac = torch.tensor(self.csr_atac[index,:].todense())\n","        return {'gex':x_gex, 'atac':x_atac, 'cell_type':self.cell_type[index]}\n","  \n","def get_dataloaders(gex_train, atac_train, cell_type_train,  gex_val, atac_val, cell_type_val):\n","    \n","    # mod2_train = mod2_train.iloc[sol_train.values.argmax(1)]\n","    # mod2_test = mod2_test.iloc[sol_test.values.argmax(1)]\n","    \n","    dataset_train = MultiomeDataset(gex_train, atac_train, cell_type_train)\n","    data_train = DataLoader(dataset_train, config.BATCH_SIZE, shuffle = True, num_workers = config.NUM_WORKERS)\n","    \n","    dataset_val = MultiomeDataset(gex_val, atac_val, cell_type_val)\n","    data_val = DataLoader(dataset_val, config.BATCH_SIZE, shuffle = False, num_workers = config.NUM_WORKERS)\n","    \n","    return data_train, data_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BO9Ki-cAkNmn"},"outputs":[],"source":["index = get_chr_index(ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/ATAC_processed.h5ad\"))"]},{"cell_type":"code","source":["gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K73UtfNLKLsS","executionInfo":{"status":"ok","timestamp":1671416565323,"user_tz":480,"elapsed":657,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}},"outputId":"80953fe6-f8e2-402b-8440-0cf449ca1d33"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["242"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77cZW1_VbJhr"},"outputs":[],"source":["batch = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/GEX_processed.h5ad\").obs['batch']\n","batch = list(batch)\n","train_id = [a for a, l in enumerate(batch) if l not in ['s2d4','s1d1']]\n","val_id =  [a for a, l in enumerate(batch) if l == 's1d1']\n","test_id = [a for a, l in enumerate(batch) if l == 's2d4']\n","\n","cell_type_all = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/GEX_processed.h5ad\").obs['cell_type']\n","\n","csr_gex = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/GEX_processed.h5ad\").layers['log_norm']\n","csr_atac = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/ATAC_processed.h5ad\").layers['log_norm']"]},{"cell_type":"code","source":["gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ohvZVM4kKaai","executionInfo":{"status":"ok","timestamp":1671416710474,"user_tz":480,"elapsed":28,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}},"outputId":"2ed133df-4991-45cb-e06e-164474ff4980"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["240"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4JBW6Qakk0k"},"outputs":[],"source":["import random\n","random.seed(0)\n","\n","idx_train = [train_id[i] for i in random.sample(range(0, 45000), 10240)]\n","gex_train = csr_gex[idx_train,:]\n","atac_train = csr_atac[idx_train,:]\n","cell_type_train = [cell_type_all[j] for j in idx_train]\n","\n","idx_val = [val_id[i] for i in random.sample(range(0, 6000), 1024)]\n","gex_val = csr_gex[idx_val,:]\n","atac_val = csr_atac[idx_val,:]\n","cell_type_val = [cell_type_all[j] for j in idx_val]\n","\n","data_train, data_val = get_dataloaders(gex_train, atac_train, cell_type_train, gex_val, atac_val, cell_type_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGKAQspKvc_V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671416842630,"user_tz":480,"elapsed":5860,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}},"outputId":"5cd3c312-41aa-47ad-fdbd-4e30c6beb562"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n","  warnings.warn('Lazy modules are a new feature under heavy development '\n"]}],"source":["criterion = bidirectTripletLoss(alpha = config.ALPHA, margin = config.MARGIN).to(config.DEVICE)\n","model = Encoder(kernel_size_gex = 100, kernel_size_atac_1 = 30, kernel_size_atac_2 = 5, index = index).to(config.DEVICE) ## CHANGED TO SMALLER KERNAL SIZE FOR ATAC\n","optimizer = torch.optim.Adam(model.parameters(), lr = config.LEARNING_RATE)\n","# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.1)\n","# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = [100], gamma = 0.1)"]},{"cell_type":"code","source":["def train(model, data_train, epochs, loss_type):\n","\n","  model.train()\n","\n","  for e in range(epochs):\n","    running_loss = 0.0\n","    running_loss_cross = 0.0\n","    running_loss_triplet = 0.0\n","    running_ct_prob = 0.0\n","    for iter, data in enumerate(data_train):\n","      gex_input = data['gex'].to(config.DEVICE)\n","      atac_input = data['atac'].to(config.DEVICE)\n","      cell_type_input = data['cell_type']\n","      # print(cell_type_input)\n","\n","      model.zero_grad()\n","      optimizer.zero_grad()\n","\n","      ### Forward\n","      gex_out_0, gex_out_1, atac_out_0, atac_out_1 = model(gex_input, atac_input)\n","\n","      ### Compute loss\n","      loss, loss_triplet, loss_cross, ct_match_prob = criterion(gex_out_0, gex_out_1, atac_out_0, atac_out_1, cell_type_input)\n","      \n","      ### Propagate loss\n","      if loss_type == \"both\":\n","        running_loss += loss.item()\n","        loss.backward()\n","        # Store other losses\n","        running_ct_prob += ct_match_prob.item()\n","        running_loss_triplet += loss_triplet.item()    \n","        running_loss_cross += loss_cross.item()  \n","        \n","      elif loss_type == \"entropy\":\n","        running_loss += loss_cross.item()\n","        loss_cross.backward()\n","        # Store other losses\n","        running_ct_prob += ct_match_prob.item()\n","        running_loss_triplet += loss_triplet.item()       \n","        running_loss_cross += loss_cross.item()  \n","\n","      elif loss_type == \"triplet\":\n","        running_loss += loss_triplet.item()\n","        loss_triplet.backward()\n","        # Store other losses\n","        running_ct_prob += ct_match_prob.item()\n","        running_loss_triplet += loss_triplet.item()    \n","        running_loss_cross += loss_cross.item()  \n","\n","      else:\n","        break\n","\n","      ### update parameters\n","      optimizer.step()\n","\n","      del gex_input\n","      del atac_input\n","      del cell_type_input\n","      torch.cuda.empty_cache()\n","      # print('Within epoch: cross_loss = ', loss_cross.item(), '; triplet_loss = ', loss_triplet.item(), '; ct_match_prob = ', ct_match_prob.item())\n","\n","    print('cross_loss = ', loss_cross.item(), '; triplet_loss = ', loss_triplet.item(), '; ct_match_prob = ', ct_match_prob.item())\n","    if (e+1) % 10 == 0: \n","      print('Epoch-{0}: lr = {1}, loss = {2}, entropy_loss = {3}, triplet loss = {4}, cell type match prob = {5}'.format(\n","          e+1, \n","          optimizer.param_groups[0]['lr'], \n","          running_loss / len(data_train), \n","          running_loss_cross / len(data_train), \n","          running_loss_triplet / len(data_train), \n","          running_ct_prob / len(data_train)\n","          )\n","      )\n","\n","    # scheduler.step()"],"metadata":{"id":"sHSqj-IvpBEv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(model, data_train, epochs = 100, loss_type = \"triplet\") "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"_TmqPFGliJKX","executionInfo":{"status":"error","timestamp":1671418005493,"user_tz":480,"elapsed":834485,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}},"outputId":"c124221f-7f87-4695-e4a4-2ae381f6d897"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cross_loss =  4.900207042694092 ; triplet_loss =  0.6797603368759155 ; ct_match_prob =  0.05097425729036331\n","cross_loss =  4.881697654724121 ; triplet_loss =  0.6276385188102722 ; ct_match_prob =  0.05448698252439499\n","cross_loss =  4.866706848144531 ; triplet_loss =  0.6524909734725952 ; ct_match_prob =  0.05606905370950699\n","cross_loss =  4.746600151062012 ; triplet_loss =  0.5818232893943787 ; ct_match_prob =  0.06194540858268738\n","cross_loss =  4.537942886352539 ; triplet_loss =  0.5772849917411804 ; ct_match_prob =  0.06884630024433136\n","cross_loss =  4.448002815246582 ; triplet_loss =  0.5775848627090454 ; ct_match_prob =  0.0724879801273346\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-27361bf29780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"triplet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-28-dacf23418635>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_train, epochs, loss_type)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;31m### Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_triplet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_cross\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct_match_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgex_out_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgex_out_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matac_out_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matac_out_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_type_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;31m### Propagate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-d03553db429b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, gex_out_0, gex_out_1, atac_out_0, atac_out_1, cell_type)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mscore_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarityScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgex_out_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgex_out_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matac_out_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matac_out_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#; print(\"score_mat:\\n\", score_mat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# print('score_mat:\\n', score_mat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mloss_triplet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriplet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mloss_cross\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrossEntropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_triplet\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_cross\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-d03553db429b>\u001b[0m in \u001b[0;36mtriplet\u001b[0;34m(self, score_mat)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# print(\"true_score dimension:\\n\", true_score.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mneg_index_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxSemiHardRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mneg_index_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxSemiHardRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# print(\"neg_index_1:\\n\", neg_index_1); print(\"neg_index_2:\\n\", neg_index_2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-d03553db429b>\u001b[0m in \u001b[0;36midxSemiHardRow\u001b[0;34m(self, score_mat)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mrow_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_mat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrue_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# each row scaled by diagonal values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mrow_logical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_scaled\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_scaled\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmargin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to sample from Trues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0miidx_sample_from\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow_logical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# indices that can be sampled from for each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m# print('iidx_sample_from', iidx_sample_from)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-d03553db429b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mrow_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_mat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrue_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# each row scaled by diagonal values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mrow_logical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_scaled\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_scaled\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmargin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to sample from Trues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0miidx_sample_from\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow_logical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# indices that can be sampled from for each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m# print('iidx_sample_from', iidx_sample_from)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-d03553db429b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mrow_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_mat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrue_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# each row scaled by diagonal values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mrow_logical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_scaled\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_scaled\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmargin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to sample from Trues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0miidx_sample_from\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow_logical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# indices that can be sampled from for each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m# print('iidx_sample_from', iidx_sample_from)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_semihard_10240cells_entropy_100epochs')"],"metadata":{"id":"v1jChVakqq3m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(model, data_train, epochs = 100, loss_type = \"triplet\") "],"metadata":{"id":"vLOl88jHxk9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_semihard_10240cells_entropy_200epochs')"],"metadata":{"id":"1erQmUcGxmNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","train(model, data_train, epochs = 100, loss_type = \"entropy\") "],"metadata":{"id":"muXpOLVYpG9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5cgSa2rj6R3"},"outputs":[],"source":["torch.save(model.state_dict(), 'drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_20480cells_entropy_100epochs')"]},{"cell_type":"code","source":["# optimizer = torch.optim.Adam(model.parameters(), lr = config.LEARNING_RATE / 10)\n","train(model, data_train, epochs = 100, loss_type = \"entropy\")"],"metadata":{"id":"0fZEWKYrpHAM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jwwf5xnnzAbo"},"outputs":[],"source":["torch.save(model.state_dict(), 'drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_20480cells_entropy_200epochs')"]},{"cell_type":"code","source":["model = Encoder(kernel_size_gex = 100, kernel_size_atac_1 = 50, kernel_size_atac_2 = 10, index = index).to(config.DEVICE)\n","model.load_state_dict(torch.load('drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_20480cells_entropy_200epochs'))"],"metadata":{"id":"vpYeQKHgA2iT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(model, data_train, epochs = 100, loss_type = \"entropy\")"],"metadata":{"id":"KvAp0t3sPyDD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_20480cells_entropy_300epochs')"],"metadata":{"id":"VnNNi7G7PyGE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adam(model.parameters(), lr = config.LEARNING_RATE / 10)\n","train(model, data_train, epochs = 100, loss_type = \"entropy\")"],"metadata":{"id":"ZuglTIPaPyIr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_20480cells_entropy_400epochs')"],"metadata":{"id":"do951ewVPyUx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(model, data_train, epochs = 100, loss_type = \"entropy\")"],"metadata":{"id":"27NhpVi-P5di"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_20480cells_entropy_500epochs')"],"metadata":{"id":"69L5CRu4P5gr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(model, data_train, epochs = 100, loss_type = \"entropy\")"],"metadata":{"id":"Iy50kXZ9P5jf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_20480cells_entropy_600epochs')"],"metadata":{"id":"XQ4z6mHrP6oH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9zQGJPY7RLK-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DsL-GcYWRLOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"s9sy9Fi7RLz1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"g9j6Fi0BRL9H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from numpy.lib.shape_base import row_stack\n","class bidirectTripletLoss2(nn.Module):\n","    r\"\"\"\n","    \n","    Output bidirectional triplet loss for two pairs of gex and atac\n","    ----------\n","    gex_0_mat: Matrix of GEX embeddings from self-attention (batch_size x embedding_size_0)\n","\n","    Returns\n","    -------\n","    loss\n","    \"\"\"\n","    def __init__(self, alpha, margin):\n","        super(bidirectTripletLoss2, self).__init__()\n","\n","        self.alpha = alpha\n","        self.margin = margin\n","        self.cross_entropy_loss = nn.CrossEntropyLoss()\n","\n","    def similarityScore(self, gex_out_0, gex_out_1, atac_out_0, atac_out_1):\n","        r\"\"\"\n","        Output similarity scores for two pairs of gex and atac\n","        ----------\n","\n","\n","        Returns\n","        -------\n","        score: batch_size * batch_size\n","        similarity score between two modalities\n","        \"\"\" \n","\n","        # print(gex_mat.size())\n","        # print(atac_mat.size())\n","\n","        # gex_mat0, gex_mat1 = torch.split(gex_mat, config.N_CHANNELS, dim = 1)\n","        # atac_mat0, atac_mat1 = torch.split(atac_mat,config.N_CHANNELS, dim = 1)\n","\n","        gex_out_0 = nn.functional.normalize(gex_out_0, dim = 1)\n","        gex_out_1 = nn.functional.normalize(gex_out_1, dim = 1)\n","        atac_out_0 = nn.functional.normalize(atac_out_0, dim = 1)\n","        atac_out_1 = nn.functional.normalize(atac_out_1, dim = 1)\n","        \n","        score = torch.mm(gex_out_0, atac_out_0.transpose(0,1)) + self.alpha * torch.mm(gex_out_1, atac_out_1.transpose(0,1))\n","        return  score.to(config.DEVICE)\n","\n","    def triplet(self, score_mat):\n","\n","        true_score = torch.diagonal(score_mat)\n","        # print(\"true_score:\\n\", true_score)\n","        # print(\"true_score dimension:\\n\", true_score.size())\n","        \n","        reduced_score_mat = score_mat - torch.diag(true_score) # set the diagnoal score to be zero\n","        \n","        neg_index_1 = torch.argmax(reduced_score_mat, dim = 1) # indices of hard negatives for GEX\n","        neg_index_2 = torch.argmax(reduced_score_mat, dim = 0) # indices of hard negatives for ATAC\n","        # print(\"neg_index_1:\\n\", neg_index_1); print(\"neg_index_2:\\n\", neg_index_2)\n","        # print(\"neg_index_1 dimension:\\n\", neg_index_1.size()); print(\"neg_index_2 dimension:\\n\", neg_index_2.size())\n","        neg_1 = score_mat[[range(config.BATCH_SIZE), neg_index_1]] # hard negatives for GEX \n","        neg_2 = score_mat[[neg_index_2, range(config.BATCH_SIZE)]] # hard negatives for ATAC\n","        # print(\"neg_1:\\n\", neg_1); print(\"neg_2:\\n\", neg_2)\n","        \n","        loss_1 = torch.max(self.margin - true_score + neg_1, torch.zeros(1, config.BATCH_SIZE).to(config.DEVICE))\n","        loss_2 = torch.max(self.margin - true_score + neg_2, torch.zeros(1, config.BATCH_SIZE).to(config.DEVICE))\n","\n","        return torch.mean(loss_1 + loss_2)\n","\n","    def crossEntropy(self, score_mat):\n","\n","        batch_size = score_mat.size()[0]\n","        target = torch.arange(batch_size)\n","\n","        loss_1 = self.cross_entropy_loss(score_mat, target.to(config.DEVICE))\n","        loss_2 = self.cross_entropy_loss(score_mat.T, target.to(config.DEVICE))\n","\n","        return 0.5 * (loss_1 + loss_2)\n","\n","    def cellMatchingProb(self, score_mat):\n","\n","        score_norm_gex = score_mat.softmax(dim = 0)\n","        score_norm_atac = score_mat.softmax(dim = 1)\n","\n","        match_probs = 0.5 * (torch.diagonal(score_norm_gex) + torch.diagonal(score_norm_atac))\n","        return torch.mean(match_probs)\n","\n","    def cellTypeMatchingProbRow(self, score_mat, cell_type):\n","\n","        # Collect list of index list for each cell type\n","        idx_in_type = collections.defaultdict(list)\n","        for i, x in enumerate(cell_type):\n","            idx_in_type[x].append(i)\n","\n","        # Compute matching probs for each cell type\n","        score_mat_norm = score_mat.softmax(dim = 0)\n","        probs = []\n","        for idx in idx_in_type.values():\n","            prob_type = 0\n","            for i in idx:\n","                prob_type += score_mat_norm[i, idx].sum()\n","            probs.append(prob_type / len(idx)) \n","\n","        # Take average of matching prob from cell types\n","        ct_match_prob = torch.tensor(probs).mean()\n","\n","        return ct_match_prob\n","\n","    def cellTypeMatchingProb(self, score_mat, cell_type):\n","        row = self.cellTypeMatchingProbRow(score_mat, cell_type) # Softmax on rows (normalize GEX)\n","        col = self.cellTypeMatchingProbRow(score_mat.T, cell_type) # Softmax on cols (normalize ATAC) \n","        \n","        return 0.5 * (row + col)\n","\n","    def forward(self, gex_out_0, gex_out_1, atac_out_0, atac_out_1, cell_type):\n","      \n","        score_mat = self.similarityScore(gex_out_0, gex_out_1, atac_out_0, atac_out_1)#; print(\"score_mat:\\n\", score_mat)\n","\n","        loss_triplet = self.triplet(score_mat)\n","        loss_cross = self.crossEntropy(score_mat)\n","        loss = loss_triplet + loss_cross\n","        # print(score_mat); print(cell_type)\n","        ct_match_prob = self.cellTypeMatchingProb(score_mat, cell_type)\n","        cell_match_prob = self.cellMatchingProb(score_mat)\n","\n","        return loss.to(config.DEVICE), loss_triplet, loss_cross, ct_match_prob, cell_match_prob"],"metadata":{"id":"HyzHLNw6-qGs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion2 =  bidirectTripletLoss2(alpha = 0.2, margin = 0.5).to(config.DEVICE)"],"metadata":{"id":"yROvSMwnQB68"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DIgJkWVD76SF"},"outputs":[],"source":["def inference(model, data_val):\n","\n","    # Initialize encoder & decoder \n","    model.eval()\n","    model.to(config.DEVICE)\n","    criterion2.to(config.DEVICE)\n","    \n","    running_loss = 0.0\n","    running_loss_triplet = 0.0\n","    running_ct_prob = 0.0  \n","    running_cell_prob = 0.0  \n","    for iter, data in enumerate(data_val):\n","      gex_input = data['gex'].to(config.DEVICE)\n","      atac_input = data['atac'].to(config.DEVICE)\n","      cell_type_input = data['cell_type']\n","\n","      ### Forward\n","      gex_out_0, gex_out_1, atac_out_0, atac_out_1 = model(gex_input, atac_input)\n","\n","      ### Compute loss\n","      loss, loss_triplet, loss_cross, ct_match_prob, cell_match_prob = criterion2(gex_out_0, gex_out_1, atac_out_0, atac_out_1, cell_type_input)\n","      \n","      running_loss += loss_cross.item()\n","      running_loss_triplet += loss_triplet.item()\n","      running_ct_prob += ct_match_prob.item()\n","      running_cell_prob += cell_match_prob.item()\n","\n","      del gex_input\n","      del atac_input\n","      del cell_type_input\n","      torch.cuda.empty_cache()\n","\n","    return running_loss / len(data_val), running_loss_triplet / len(data_val), running_ct_prob / len(data_val), running_cell_prob / len(data_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKQlEJeowARe"},"outputs":[],"source":["# model = Encoder(kernel_size_gex = 100, kernel_size_atac_1 = 50, kernel_size_atac_2 = 10, index = index).to(config.DEVICE)\n","# model.load_state_dict(torch.load('drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_20480cells_entropy_100epochs'))"]},{"cell_type":"code","source":["loss, loss_triplet, ct_match_prob, cell_match_prob = inference(model, data_val)\n","print('loss = {0}, triplet loss = {1}, cell type match prob = {2}, cell match prob = {3}'.format(loss, loss_triplet, ct_match_prob, cell_match_prob))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SqMJ-oa31XZ0","executionInfo":{"status":"ok","timestamp":1671176137149,"user_tz":480,"elapsed":4272,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}},"outputId":"03e325e3-f95c-4e14-e036-d8ee50efccd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loss = 4.853066444396973, triplet loss = 1.2009247988462448, cell type match prob = 0.057939017191529274, cell match prob = 0.007810505630914122\n"]}]},{"cell_type":"code","source":["loss, loss_triplet, ct_match_prob, cell_match_prob = inference(model, data_val)\n","print('loss = {0}, triplet loss = {1}, cell type match prob = {2}'.format(loss, loss_triplet, ct_match_prob))"],"metadata":{"id":"GihfY73gjp2w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671160412959,"user_tz":480,"elapsed":4198,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}},"outputId":"a14f778e-5ed0-48fa-8493-aa145b1f2639"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["true_score:\n"," tensor([0.7585, 1.0858, 1.1351, 1.1777, 0.8089, 1.1733, 1.1585, 1.1663, 1.1601,\n","        0.8108, 1.1258, 1.1209, 1.1727, 1.1229, 1.1380, 0.8305, 1.1894, 1.1598,\n","        1.0420, 1.1904, 1.1429, 1.1172, 1.0640, 1.1743, 0.5028, 1.1472, 1.1735,\n","        1.1771, 1.0118, 0.9869, 1.1901, 1.0607, 0.6916, 1.1897, 1.1828, 1.1937,\n","        1.0343, 1.1211, 1.1178, 0.9611, 1.1633, 1.1524, 1.0532, 0.9729, 1.1839,\n","        1.1698, 1.1419, 1.1378, 1.1938, 1.1422, 1.1796, 1.1697, 1.0853, 1.1789,\n","        1.1655, 1.1016, 0.6504, 1.1904, 1.0595, 1.0931, 1.0607, 1.1717, 1.1751,\n","        1.1441, 1.1582, 0.8834, 1.1948, 1.1763, 1.1717, 1.1340, 1.1404, 1.0920,\n","        1.1740, 0.7811, 1.1483, 0.3905, 1.1831, 0.8829, 1.1926, 1.1799, 1.1629,\n","        1.1717, 1.1870, 1.0811, 1.1881, 1.1741, 1.0709, 1.1747, 1.1841, 1.1680,\n","        1.0474, 1.1693, 1.0864, 1.1707, 1.0862, 1.1816, 1.1480, 1.0079, 1.0609,\n","        1.1297, 1.1150, 1.1163, 1.1340, 1.1047, 1.1537, 1.0986, 1.1049, 1.1039,\n","        1.1690, 1.1934, 1.1840, 1.1312, 1.1644, 1.1193, 1.1368, 1.1191, 1.1862,\n","        1.1963, 1.0649, 0.6937, 0.9223, 1.1764, 1.1587, 1.1889, 1.0937, 1.1750,\n","        0.9768, 1.1488], device='cuda:0', grad_fn=<DiagonalBackward0>)\n","true_score:\n"," tensor([1.1851, 1.1760, 1.1927, 1.0834, 1.1885, 1.1921, 1.1637, 1.1929, 0.6979,\n","        1.1819, 0.3163, 0.8955, 0.7924, 1.1942, 1.0148, 1.1893, 1.0613, 1.0161,\n","        1.1741, 1.1142, 1.1667, 1.1584, 1.1532, 1.1108, 1.1739, 1.0859, 1.1170,\n","        1.1863, 0.9696, 1.1699, 1.1358, 0.9170, 1.1589, 1.1097, 0.6141, 1.1701,\n","        1.0505, 0.8659, 1.1751, 1.1766, 1.1086, 1.1275, 0.8590, 0.9029, 1.0064,\n","        1.1694, 1.1687, 1.1883, 1.0476, 0.9423, 1.1472, 1.1504, 1.1805, 1.1882,\n","        1.1423, 1.1355, 1.1813, 1.1559, 1.0333, 0.6932, 1.1271, 1.1523, 1.1834,\n","        1.1824, 1.1954, 1.0271, 1.1467, 1.1935, 1.1856, 1.1515, 1.1762, 1.1799,\n","        1.1819, 1.1316, 1.1668, 1.1039, 1.1026, 1.1714, 1.0241, 1.0174, 1.1853,\n","        1.1332, 0.9914, 0.8608, 1.1800, 1.1542, 1.0162, 1.1792, 0.8630, 1.1846,\n","        1.1965, 1.1951, 1.1642, 1.1571, 0.9595, 1.1682, 1.1677, 1.1841, 1.1671,\n","        1.0848, 1.1073, 1.0493, 1.1941, 1.0351, 1.1651, 1.1956, 1.1902, 1.1848,\n","        1.1901, 1.0690, 1.1565, 1.1762, 0.7828, 1.1478, 1.1409, 1.1970, 1.0601,\n","        0.9842, 1.1894, 1.1648, 0.5854, 1.0856, 1.1896, 1.1897, 1.1906, 1.1868,\n","        1.1862, 1.1261], device='cuda:0', grad_fn=<DiagonalBackward0>)\n","true_score:\n"," tensor([ 1.1825,  1.1865,  1.1210,  1.1764,  1.1519,  1.1297,  1.1609,  1.1777,\n","         0.8452,  1.1440,  1.1900,  1.1704,  1.1806,  1.1580,  1.1025,  1.1048,\n","         1.1718,  1.1901,  1.1063,  1.0785,  1.1624,  1.0697,  1.1456,  1.1592,\n","         0.9630,  1.0966,  1.1164,  0.4820,  1.0272,  1.1955,  1.1806,  1.0297,\n","         1.1704,  0.7860,  1.1553,  1.1692,  1.1534,  1.1797,  0.9807,  1.0659,\n","         1.1633,  0.9899,  0.7269,  1.1776,  0.9089,  1.0956,  1.1727,  1.1335,\n","         1.0959,  1.1346,  1.1172,  1.1875,  1.0494,  1.1202,  1.1437,  1.1941,\n","         1.1433,  1.0227,  1.1876,  1.1621,  1.1496,  1.1107,  1.1078,  1.1547,\n","         1.0861,  1.1390,  1.1110,  0.6961,  1.1183,  1.1766,  1.0096,  1.1899,\n","         1.1740,  1.1820,  1.1945,  0.7608,  1.1746,  0.7561,  1.1787,  1.1439,\n","         1.1625,  1.0913,  1.0248,  1.1112,  1.1887,  1.1084,  1.1889,  1.1577,\n","         1.1677,  1.1829,  0.7646,  1.1115,  1.1543,  1.1837,  1.1908,  1.1889,\n","         1.1368,  1.0873,  1.1783,  1.1645,  1.1439,  1.1736,  1.1953,  1.1464,\n","         1.1838,  1.1363,  1.1566,  1.1342,  1.1874,  0.8746,  1.1714,  1.1449,\n","         1.1485,  1.1301,  1.1830,  1.1268,  1.1177,  1.1678,  1.1038,  1.1483,\n","        -0.0722,  1.1295,  1.1616,  1.1675,  1.1552,  1.1870,  1.1091,  1.0540],\n","       device='cuda:0', grad_fn=<DiagonalBackward0>)\n","true_score:\n"," tensor([ 1.1112,  1.1891,  1.1317,  1.1273,  1.1803,  0.9971,  0.8069,  1.1859,\n","         1.1222,  1.1698,  1.1290,  1.1842,  1.1253,  1.1620,  0.9948,  1.1037,\n","         1.0631,  1.1064,  1.1889,  1.1766,  1.1753,  1.1827,  1.1617,  0.9484,\n","         1.0520,  1.1785,  1.0856,  0.8288,  1.1400,  1.1413,  1.1377,  1.0076,\n","         1.0961,  1.1600,  1.1927,  0.8735,  1.1408,  0.8388,  1.1649,  0.8410,\n","         0.7980, -0.1633,  0.4691,  1.1939,  1.1955,  1.0640,  1.1645,  1.0838,\n","         1.1795,  1.0986,  1.1617, -0.1402,  1.0692,  1.1217,  0.4787,  1.1433,\n","         1.1422,  1.1808,  1.1916,  1.1901,  1.1837,  1.1794,  1.1920,  1.0189,\n","         1.1885,  1.1645,  1.1439,  1.1468,  1.1177,  1.1647,  1.0114,  1.1705,\n","         1.1665,  0.8990,  1.1905,  1.1893,  1.1806,  1.0558,  1.1294,  1.1972,\n","         1.1543,  0.9405,  1.1741,  1.0483,  1.1658,  1.1602,  0.9852,  1.0921,\n","         1.1496,  0.6088,  1.1436,  1.1646,  1.1511,  1.1889,  1.0857,  1.1777,\n","         0.9946,  0.9858,  1.1471,  1.0661,  0.6889,  1.1657,  1.1540,  1.1880,\n","         1.1709,  1.1624,  1.1498,  1.0462,  1.1682,  1.0729,  1.1343,  1.1818,\n","         1.1244, -0.3473,  1.1811,  1.1920,  1.1526,  1.1973,  1.1766,  1.1704,\n","         1.0453,  0.9400,  1.1759,  1.1873,  1.1033,  1.1139,  1.1738,  1.1851],\n","       device='cuda:0', grad_fn=<DiagonalBackward0>)\n","true_score:\n"," tensor([ 1.1810,  1.0992,  1.1689, -0.2314,  1.1516,  1.1513,  1.0683,  1.1869,\n","         1.1005,  1.0929,  0.9910,  1.0839,  1.1396,  1.1651,  1.1879,  1.1605,\n","         1.1584,  1.1795,  0.9873,  1.1315,  1.1643,  1.1057,  0.8097,  1.1808,\n","         1.1266,  1.1649,  0.6854,  1.1538,  0.6644,  0.5346,  1.0156,  1.1432,\n","         1.1094,  1.1970,  1.0678,  1.1068,  0.9256,  1.1429,  1.0683,  1.1326,\n","         1.1383,  1.0525,  1.1432,  1.1192,  1.0272,  1.1151,  0.9831,  1.1843,\n","         1.1398,  1.1841,  1.1965,  1.1072,  1.1703,  1.0964,  1.1568,  1.1833,\n","         1.1374,  1.1045,  1.1929,  1.1107,  1.1733,  1.1822,  0.8282,  1.1405,\n","         1.1305,  0.9113,  0.9080,  1.1563,  1.1024,  0.8242,  1.1854,  1.1832,\n","         1.1752,  1.1870,  1.1816,  1.1327,  0.7123,  1.1903,  0.8064,  1.0730,\n","         1.1733,  1.0823,  1.1936,  0.8961,  1.1331,  1.1711,  1.1489,  1.1974,\n","         1.1916,  0.7898,  1.1956,  1.1409,  1.1630,  1.1481,  0.6784,  1.1793,\n","         1.1539,  1.1547,  1.1310,  0.6936,  0.8294,  1.1911,  1.1653,  1.1877,\n","         0.7569,  1.1417,  1.1942,  1.1801,  1.1487,  0.8566,  1.1631,  1.1745,\n","         1.1461,  1.1531,  1.0117,  1.0145,  1.1864,  1.0202,  1.1873,  1.1400,\n","         1.0951,  1.0705,  1.1651,  1.0837,  1.0398,  1.1909,  1.1684,  1.1591],\n","       device='cuda:0', grad_fn=<DiagonalBackward0>)\n","true_score:\n"," tensor([1.1347, 1.1080, 1.1854, 0.8163, 1.0307, 0.8385, 1.1362, 1.1826, 1.1430,\n","        1.1535, 1.1924, 1.1424, 1.1341, 1.0451, 1.1569, 1.1876, 1.1862, 1.0564,\n","        1.1919, 1.1324, 0.8824, 1.1616, 1.1255, 1.0071, 1.1273, 0.9575, 1.1863,\n","        1.1708, 1.1860, 1.1329, 1.1474, 1.1608, 1.1163, 1.1386, 0.8338, 1.1858,\n","        1.1824, 1.1302, 1.1713, 1.1908, 1.0599, 1.1763, 1.0186, 1.1434, 1.1607,\n","        1.1385, 1.1884, 1.1372, 1.1808, 1.1812, 1.1851, 1.1599, 1.1719, 1.1483,\n","        1.1714, 1.1790, 1.0960, 1.1858, 1.1169, 1.1876, 1.1644, 1.1756, 1.1516,\n","        1.1885, 0.7747, 1.0997, 1.1719, 1.0878, 0.9205, 1.0201, 1.1722, 1.0505,\n","        1.1450, 0.9647, 1.1233, 1.1396, 1.1865, 1.1938, 1.1073, 1.1449, 0.7838,\n","        1.1708, 1.1690, 1.1818, 1.1882, 1.1125, 1.0258, 1.1540, 1.1965, 1.1571,\n","        1.1683, 1.1255, 1.1842, 1.0897, 1.1459, 1.0110, 1.0826, 1.1909, 1.1916,\n","        1.1714, 1.1535, 0.7777, 1.0383, 0.7695, 1.1717, 1.1248, 1.1841, 1.0984,\n","        1.1085, 1.0302, 1.1081, 1.1127, 1.1518, 1.1191, 0.4863, 0.9640, 0.8393,\n","        1.1863, 1.0736, 1.1794, 1.1800, 1.1835, 1.1812, 1.1701, 1.1154, 1.1610,\n","        1.1339, 1.1877], device='cuda:0', grad_fn=<DiagonalBackward0>)\n","true_score:\n"," tensor([1.1924, 1.1786, 0.9945, 1.0215, 1.0970, 1.1664, 1.1806, 0.9415, 1.1759,\n","        1.1473, 1.1624, 1.1554, 1.1515, 1.1898, 1.1870, 1.1479, 1.1721, 0.9167,\n","        1.1839, 1.1454, 1.1799, 1.1477, 1.1894, 1.0600, 1.0365, 1.0932, 1.1721,\n","        1.0832, 1.1118, 1.1330, 1.1703, 1.1575, 1.1683, 1.1045, 1.1485, 1.1654,\n","        1.0469, 1.0600, 1.1294, 1.1083, 1.1838, 1.0531, 1.1828, 1.1591, 1.0584,\n","        1.1711, 1.1045, 1.1849, 1.1729, 1.1626, 1.0683, 1.1853, 1.1885, 1.1134,\n","        1.1786, 1.1731, 1.0260, 1.1642, 1.1654, 1.1404, 1.1630, 1.0457, 1.0919,\n","        0.9102, 1.1646, 0.2515, 1.1066, 1.1339, 1.1948, 1.1486, 1.1386, 1.1292,\n","        1.1719, 1.0593, 1.0490, 1.1947, 0.8088, 1.1324, 1.1054, 1.1923, 1.1221,\n","        1.1577, 1.1907, 1.1747, 1.1853, 1.0995, 0.8997, 1.0688, 0.9285, 1.1778,\n","        1.1872, 0.9633, 1.1343, 1.1621, 1.1729, 1.1310, 1.1809, 1.1730, 1.1112,\n","        1.0367, 1.1871, 0.6668, 1.0919, 1.1827, 1.1654, 1.1647, 0.7653, 0.6486,\n","        1.0215, 1.1503, 1.0677, 1.0943, 1.0799, 1.1311, 1.1876, 1.0476, 1.1421,\n","        1.1947, 1.1735, 1.0911, 0.8486, 1.1442, 1.1345, 1.1557, 1.1749, 1.1849,\n","        0.9494, 1.0037], device='cuda:0', grad_fn=<DiagonalBackward0>)\n","true_score:\n"," tensor([1.1440, 1.1365, 1.0707, 1.1211, 1.1632, 0.9108, 1.1961, 1.1664, 1.1775,\n","        1.1420, 1.0277, 1.0236, 1.1964, 1.1772, 1.0408, 1.1903, 0.7298, 1.0576,\n","        1.1821, 1.1924, 1.1734, 1.0838, 1.1829, 1.1501, 1.1199, 1.0872, 1.1529,\n","        1.1832, 1.1440, 1.1935, 0.9876, 0.8765, 1.1641, 1.1582, 1.0428, 1.1256,\n","        1.0535, 1.1878, 1.1629, 0.8280, 1.1507, 0.9624, 1.1895, 1.1909, 1.1776,\n","        1.1567, 1.1921, 0.9742, 1.1866, 1.1124, 1.1822, 1.1348, 1.1716, 1.1748,\n","        1.1840, 1.1192, 1.1657, 1.1210, 1.1838, 1.1863, 1.1869, 0.8786, 1.1407,\n","        1.1391, 1.1608, 1.1843, 1.1010, 1.1181, 1.1692, 1.1869, 1.1759, 1.1655,\n","        1.0837, 1.1710, 1.1808, 1.0744, 1.1735, 1.1752, 1.1453, 1.1790, 1.1589,\n","        1.1517, 0.4950, 1.1528, 1.1905, 1.0896, 1.1784, 0.6796, 1.0944, 0.8502,\n","        0.8026, 1.1678, 1.1293, 0.9767, 1.0074, 1.1526, 1.0016, 1.1007, 1.1869,\n","        1.1611, 1.0571, 1.1491, 1.1917, 1.0645, 0.5030, 1.0885, 1.1911, 1.1330,\n","        1.1532, 1.1922, 1.0970, 0.7907, 1.1377, 1.1853, 0.2331, 1.1537, 1.0831,\n","        1.1296, 1.1874, 1.1458, 1.0763, 1.0493, 1.1814, 1.1976, 1.0094, 1.1415,\n","        1.1148, 0.7953], device='cuda:0', grad_fn=<DiagonalBackward0>)\n","loss = 4.023020625114441, triplet loss = 1.1642208695411682, cell type match prob = 0.1317867562174797\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6d-VNU7IQXse"},"outputs":[],"source":["# model = Encoder(kernel_size_gex = 100, kernel_size_atac_1 = 50, kernel_size_atac_2 = 10, index = index).to(config.DEVICE)\n","# model.load_state_dict(torch.load('drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_20480cells_entropy_200epochs'))"]},{"cell_type":"code","source":["loss, loss_triplet, ct_match_prob = inference(model, data_val)\n","print('loss = {0}, triplet loss = {1}, cell type match prob = {2}'.format(loss, loss_triplet, ct_match_prob))"],"metadata":{"id":"TVIog7EvQZU7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KiM_RN4-QX6I"},"outputs":[],"source":["model = Encoder(kernel_size_gex = 100, kernel_size_atac_1 = 50, kernel_size_atac_2 = 10, index = index).to(config.DEVICE)\n","model.load_state_dict(torch.load('drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_20480cells_entropy_300epochs'))"]},{"cell_type":"code","source":["loss, loss_triplet, ct_match_prob = inference(model, data_val)\n","print('loss = {0}, triplet loss = {1}, cell type match prob = {2}'.format(loss, loss_triplet, ct_match_prob))"],"metadata":{"id":"Q0QJdHe7QZde"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wRbYOavQX-K"},"outputs":[],"source":["model = Encoder(kernel_size_gex = 100, kernel_size_atac_1 = 50, kernel_size_atac_2 = 10, index = index).to(config.DEVICE)\n","model.load_state_dict(torch.load('drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_20480cells_entropy_400epochs'))"]},{"cell_type":"code","source":["loss, loss_triplet, ct_match_prob = inference(model, data_val)\n","print('loss = {0}, triplet loss = {1}, cell type match prob = {2}'.format(loss, loss_triplet, ct_match_prob))"],"metadata":{"id":"GNjwP21QQZgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nRFTp7vLQYIM"},"outputs":[],"source":["model = Encoder(kernel_size_gex = 100, kernel_size_atac_1 = 50, kernel_size_atac_2 = 10, index = index).to(config.DEVICE)\n","model.load_state_dict(torch.load('drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_20480cells_entropy_500epochs'))"]},{"cell_type":"code","source":["loss, loss_triplet, ct_match_prob = inference(model, data_val)\n","print('loss = {0}, triplet loss = {1}, cell type match prob = {2}'.format(loss, loss_triplet, ct_match_prob))"],"metadata":{"id":"XQao0lQ6N521"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Encoder(kernel_size_gex = 100, kernel_size_atac_1 = 50, kernel_size_atac_2 = 10, index = index).to(config.DEVICE)\n","model.load_state_dict(torch.load('drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_20480cells_entropy_600epochs'))"],"metadata":{"id":"bR6xFt0rOcPM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss, loss_triplet, ct_match_prob = inference(model, data_val)\n","print('loss = {0}, triplet loss = {1}, cell type match prob = {2}'.format(loss, loss_triplet, ct_match_prob))"],"metadata":{"id":"zgDj0A6WOcSI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eYVyj2xzOcUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"q6eZFUo8Ocb2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0Tua8h4-Ocey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FzwJ0f_9Ochm"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}